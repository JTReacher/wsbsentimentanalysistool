{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 50,
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from pandas.io import sql\n",
                "from jsonhandlers import *\n",
                "from sqlalchemy import create_engine\n",
                "import spacy\n",
                "from spacy import displacy\n",
                "from yahoo_fin import stock_info as si\n",
                "from yahoo_fin import *\n",
                "import re\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "paths = os.listdir('/workspace/JSONFiles/submissionsSplit')\n",
                "sql_Engine = create_engine('mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "\n",
                "for file in paths:\n",
                "    file_path = '/workspace/JSONFiles/submissionsSplit/' + file\n",
                "    print(file_path)\n",
                "    with open(file_path, 'r', encoding='utf-8') as fileobject:\n",
                "        df = pd.read_json(fileobject, lines=True)\n",
                "        df_filtered = df.filter(\n",
                "            ['created_utc', 'retrieved_on', 'author_created_utc', 'author', 'title', 'selftext'])\n",
                "        df_filtered.to_sql(con=db_connection, name=\"RedditPosts\", if_exists=\"append\")    \n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "malformed_list = []\n",
                "for file in paths:\n",
                "    path = '/workspace/JSONFiles/submissionsSplit/' + file\n",
                "    malformed_list.append(find_malformed_lines(path))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "sentence = \"MSFT is looking at buying U.K. startup for $1 billion\"\n",
                "\n",
                "doc = nlp(sentence)\n",
                "displacy.render(doc, style='ent')\n",
                "type(doc)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "NLP_results = []\n",
                "with open('/workspace/JSONFiles/submissionsSplit/nlpdemofile.jsonl') as file:\n",
                "    for line in file:\n",
                "        json_data = json.loads(line.rstrip('\\n|\\r'))\n",
                "        doc = nlp(json_data.get('title'))\n",
                "        NLP_results.append(doc)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for doc in NLP_results[:20]:\n",
                "    displacy.render(doc, style='ent')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "source": [
                "tickers_set = get_all_tickers_yahoo_finance()\n",
                "tickers_list = list(tickers_set)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "source": [
                "tickers_cleaned = []\n",
                "for ticker in tickers_list:\n",
                "    clean_ticker = []\n",
                "    for letter in ticker:\n",
                "        if letter == '.' or letter == '$':\n",
                "          continue\n",
                "        else:\n",
                "          clean_ticker.append(letter)\n",
                "    tickers_cleaned.append(''.join(clean_ticker))\n",
                "        \n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "tickers_cleaned"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for i in tickers_dict:\n",
                "    ni = i.split('.')[0]\n",
                "    print(ni)\n",
                "    tickers_dict[ni] = si.get_quote_data(ni)['shortName']\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.14 64-bit"
        },
        "interpreter": {
            "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}