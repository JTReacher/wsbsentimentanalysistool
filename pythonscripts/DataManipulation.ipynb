{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from pandas.io import sql\n",
                "from jsonhandlers import *\n",
                "from sqlalchemy import engine\n",
                "import sqlalchemy as db\n",
                "import spacy\n",
                "from spacy import displacy\n",
                "from yahoo_fin import stock_info as si\n",
                "from yahoo_fin import *\n",
                "from fuzzywuzzy import fuzz\n",
                "from fuzzywuzzy import process\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "paths = os.listdir('/workspace/JSONFiles/submissionsSplit')\n",
                "sql_Engine = create_engine('mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "\n",
                "for file in paths:\n",
                "    file_path = '/workspace/JSONFiles/submissionsSplit/' + file\n",
                "    print(file_path)\n",
                "    with open(file_path, 'r', encoding='utf-8') as fileobject:\n",
                "        df = pd.read_json(fileobject, lines=True)\n",
                "        df_filtered = df.filter(\n",
                "            ['created_utc', 'retrieved_on', 'author_created_utc', 'author', 'title', 'selftext'])\n",
                "        df_filtered.to_sql(con=db_connection, name=\"RedditPosts\", if_exists=\"append\")    \n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "malformed_list = []\n",
                "for file in paths:\n",
                "    path = '/workspace/JSONFiles/submissionsSplit/' + file\n",
                "    malformed_list.append(find_malformed_lines(path))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "sentence = \"MSFT is looking at buying U.K. startup for $1 billion\"\n",
                "\n",
                "doc = nlp(sentence)\n",
                "displacy.render(doc, style='ent')\n",
                "type(doc)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "NLP_results = []\n",
                "with open('/workspace/JSONFiles/submissionsSplit/nlpdemofile.jsonl') as file:\n",
                "    for line in file:\n",
                "        json_data = json.loads(line.rstrip('\\n|\\r'))\n",
                "        doc = nlp(json_data.get('title'))\n",
                "        NLP_results.append(doc)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for doc in NLP_results[:20]:\n",
                "    displacy.render(doc, style='ent')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "tickers_set = get_all_tickers_yahoo_finance()\n",
                "tickers_list = list(tickers_set)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "tickers_cleaned = []\n",
                "for ticker in tickers_list:\n",
                "    clean_ticker = []\n",
                "    for letter in ticker:\n",
                "        if letter == '.' or letter == '$':\n",
                "          continue\n",
                "        else:\n",
                "          clean_ticker.append(letter)\n",
                "    tickers_cleaned.append(''.join(clean_ticker))\n",
                "del tickers_cleaned[0]\n",
                "        \n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "tickers_names_dict = dict.fromkeys(tickers_cleaned)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 129,
            "source": [
                "counter = 0\n",
                "for i in tickers_names_dict:\n",
                "    try:\n",
                "        tickers_names_dict[i] = si.get_quote_data(i)['shortName']\n",
                "    except:\n",
                "      counter += 1\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "tickers_names_dict"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 134,
            "source": [
                "#Check how much we lost\n",
                "counter = 0 \n",
                "for i in tickers_names_dict:\n",
                "    if tickers_names_dict[i] == None:\n",
                "        counter += 1\n",
                "print(counter)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "940\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Get it to SQL via dataframe\n",
                "df_tickers_names = pd.DataFrame.from_dict(tickers_names_dict.items())\n",
                "df_tickers_names.columns = ['Ticker', 'Company']\n",
                "sql_Engine = create_engine(\n",
                "    'mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "df_tickers_names.to_sql(\n",
                "    con=db_connection, name=\"TickersCompanies\", if_exists=\"append\")\n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "#Get sample data. Can use the cursor to do this iteratively for the whole thing?\n",
                "sql_Engine = db.create_engine('mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "#This is in list/tuple form\n",
                "tickers_names_dict = db_connection.execute(\"SELECT * FROM TickersCompanies\").fetchall()\n",
                "sample_data = db_connection.execute(\"SELECT RedditPosts.index, title, selftext FROM RedditPosts\").fetchall()\n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "#retain row number, unify title and selftext. Extract anything in all caps and anything that starts with $ sign. Ignore blacklist words. Frequency of ticker mention might be a good filter. Prioritise mention in title over text.\n",
                "blacklist = [\"IPO\", \"YOLO\", \"RIP\", \"None\", \"Earnings\", \"Wallstreet\"]\n",
                "#current_row = 0\n",
                "extracted_tickers_names = {}\n",
                "counter = 0 \n",
                "for row in sample_data:\n",
                "    #current_row = row[0]\n",
                "    probable_tickers_and_names = []\n",
                "    all_text = str(row[1]) + \" \" + str(row[2])\n",
                "    all_text = all_text.split()\n",
                "    for word in all_text:\n",
                "        if word[0] == \"$\":\n",
                "            probable_tickers_and_names.append(word)\n",
                "        elif word.isupper():\n",
                "            probable_tickers_and_names.append(word)\n",
                "        elif word.istitle() and word not in blacklist:\n",
                "            probable_tickers_and_names.append(word)\n",
                "        else:\n",
                "            break\n",
                "    #add key value pair to dictionary row number key then list\n",
                "    extracted_tickers_names[counter] = probable_tickers_and_names\n",
                "    counter += 1\n",
                "\n",
                "for x in range(200):\n",
                "    print(extracted_tickers_names[x])\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 201,
            "source": [
                "# #Matching for tickers only. Names need to be partial ratio\n",
                "# possible_matches = []\n",
                "# for word in sample_data:\n",
                "#     for item in tickers_names_dict:\n",
                "#         similarity = fuzz.ratio(word, item)\n",
                "#         if similarity > 90:\n",
                "#             possible_matches.append([word, item, similarity ])\n",
                "\n",
                "#Retain index, ticker and name for extraction in a new list for upload to SQL\n",
                "\n",
                "# extracted_tickers_names = []\n",
                "# for row in sample_data[0:20]:\n",
                "#     possible_matches = [row[0]]\n",
                "#     for word in [row[1].split()]:\n",
                "#         for item in tickers_names_dict:\n",
                "#             similarity = fuzz.ratio(word, item[1])\n",
                "#             #if similarity > 90:\n",
                "#             possible_matches.append([word, item[1], similarity])\n",
                "#     extracted_tickers_names.append(possible_matches)\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.14 64-bit"
        },
        "interpreter": {
            "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}