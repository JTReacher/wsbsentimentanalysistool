{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 25,
            "source": [
                "import os\n",
                "import pandas as pd\n",
                "from pandas.io import sql\n",
                "from jsonhandlers import *\n",
                "from sqlalchemy import engine\n",
                "import sqlalchemy as db\n",
                "import spacy\n",
                "from spacy import displacy\n",
                "from yahoo_fin import stock_info as si\n",
                "from yahoo_fin import *\n",
                "from fuzzywuzzy import fuzz\n",
                "from fuzzywuzzy import process\n",
                "from english_words import english_words_set\n",
                "import string\n",
                "import re\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "paths = os.listdir('/workspace/JSONFiles/submissionsSplit')\n",
                "sql_Engine = create_engine('mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "\n",
                "for file in paths:\n",
                "    file_path = '/workspace/JSONFiles/submissionsSplit/' + file\n",
                "    print(file_path)\n",
                "    with open(file_path, 'r', encoding='utf-8') as fileobject:\n",
                "        df = pd.read_json(fileobject, lines=True)\n",
                "        df_filtered = df.filter(\n",
                "            ['created_utc', 'retrieved_on', 'author_created_utc', 'author', 'title', 'selftext'])\n",
                "        df_filtered.to_sql(con=db_connection, name=\"RedditPosts\", if_exists=\"append\")    \n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "malformed_list = []\n",
                "for file in paths:\n",
                "    path = '/workspace/JSONFiles/submissionsSplit/' + file\n",
                "    malformed_list.append(find_malformed_lines(path))\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "sentence = \"MSFT is looking at buying U.K. startup for $1 billion\"\n",
                "\n",
                "doc = nlp(sentence)\n",
                "displacy.render(doc, style='ent')\n",
                "type(doc)\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "source": [
                "NLP_results = []\n",
                "with open('/workspace/JSONFiles/submissionsSplit/nlpdemofile.jsonl') as file:\n",
                "    for line in file:\n",
                "        json_data = json.loads(line.rstrip('\\n|\\r'))\n",
                "        doc = nlp(json_data.get('title'))\n",
                "        NLP_results.append(doc)\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "for doc in NLP_results[:20]:\n",
                "    displacy.render(doc, style='ent')\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "tickers_set = get_all_tickers_yahoo_finance()\n",
                "tickers_list = list(tickers_set)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "tickers_cleaned = []\n",
                "for ticker in tickers_list:\n",
                "    clean_ticker = []\n",
                "    for letter in ticker:\n",
                "        if letter == '.' or letter == '$':\n",
                "          continue\n",
                "        else:\n",
                "          clean_ticker.append(letter)\n",
                "    tickers_cleaned.append(''.join(clean_ticker))\n",
                "del tickers_cleaned[0]\n",
                "        \n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "tickers_names_dict = dict.fromkeys(tickers_cleaned)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 129,
            "source": [
                "counter = 0\n",
                "for i in tickers_names_dict:\n",
                "    try:\n",
                "        tickers_names_dict[i] = si.get_quote_data(i)['shortName']\n",
                "    except:\n",
                "      counter += 1\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 134,
            "source": [
                "#Check how much we lost\n",
                "counter = 0 \n",
                "for i in tickers_names_dict:\n",
                "    if tickers_names_dict[i] == None:\n",
                "        counter += 1\n",
                "print(counter)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "940\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "#Get it to SQL via dataframe\n",
                "df_tickers_names = pd.DataFrame.from_dict(tickers_names_dict.items())\n",
                "df_tickers_names.columns = ['Ticker', 'Company']\n",
                "sql_Engine = create_engine(\n",
                "    'mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "df_tickers_names.to_sql(\n",
                "    con=db_connection, name=\"TickersCompanies\", if_exists=\"append\")\n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "#Get sample data. Can use the cursor to do this iteratively for the whole thing?\n",
                "sql_Engine = db.create_engine('mysql+pymysql://mysql:mysql@localhost:3306/test_db')\n",
                "db_connection = sql_Engine.connect()\n",
                "#This is in list/tuple form\n",
                "tickers_names_dict = db_connection.execute(\"SELECT * FROM TickersCompanies\").fetchall()\n",
                "sample_data = db_connection.execute(\"SELECT RedditPosts.index, title, selftext FROM RedditPosts\").fetchall()\n",
                "db_connection.close()\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "#retain row number, unify title and selftext. Extract anything in all caps and anything that starts with $ sign. Ignore blacklist words. Frequency of ticker mention might be a good filter. Prioritise mention in title over text.\n",
                "#TODO: This could still work if you discard any with over x results\n",
                "\n",
                "#instantiate dictionary object\n",
                "dictionary=english_words_set\n",
                "\n",
                "blacklist = [\"IPO\", \"YOLO\", \"RIP\", \"None\", \"Earnings\", \"Wallstreet\", \"MOON\", \"RIP\"]\n",
                "#current_row = 0\n",
                "extracted_tickers_names = {}\n",
                "counter = 0 \n",
                "for row in sample_data:\n",
                "    #current_row = row[0]\n",
                "    probable_tickers_and_names = []\n",
                "    all_text = str(row[1]) + \" \" + str(row[2])\n",
                "    all_text = all_text.split()\n",
                "    for word in all_text:\n",
                "        if word[0] == \"$\" and word not in blacklist and word.lower() not in dictionary:\n",
                "            probable_tickers_and_names.append(word)\n",
                "        elif word.isupper() and word not in blacklist and word.lower() not in dictionary:\n",
                "            probable_tickers_and_names.append(word)\n",
                "        elif word.istitle() and word not in blacklist and word.lower() not in dictionary:\n",
                "            probable_tickers_and_names.append(word)\n",
                "        else:\n",
                "            break\n",
                "    \n",
                "\n",
                "    #add key value pair to dictionary row number key then list\n",
                "    extracted_tickers_names[counter] = probable_tickers_and_names\n",
                "    counter += 1\n",
                "\n",
                "for x in range(200):\n",
                "    print(extracted_tickers_names[x])\n",
                "\n",
                "# Delete $ from words in probabl tickers, then uppercase them all, then make a set\n",
                "# add a check to see if word is in English dictionary and exclude if it is\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['$MSFT']\n",
                        "[]\n",
                        "['$TOPS']\n",
                        "[]\n",
                        "['PBIB?', 'Ladies']\n",
                        "[]\n",
                        "['{BUY}']\n",
                        "['Whats']\n",
                        "['[Discussion]']\n",
                        "[]\n",
                        "[]\n",
                        "['Dgaz']\n",
                        "[]\n",
                        "['Calling']\n",
                        "[]\n",
                        "['EXEL', 'Posts']\n",
                        "['Analysis:']\n",
                        "['Vivint']\n",
                        "['AMD', 'UPDATE:']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$PYPL:']\n",
                        "['AMBA']\n",
                        "['EXXI', 'YOLO?']\n",
                        "['$rgse', 'Undervalued']\n",
                        "['$CYCC:']\n",
                        "['CODI', '$16.65', '--NEW']\n",
                        "['Picks']\n",
                        "['$NXGA']\n",
                        "[]\n",
                        "['$AVXL']\n",
                        "[]\n",
                        "[]\n",
                        "['HJOE']\n",
                        "['Jordan', 'Spieth']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$WLCDF']\n",
                        "['JDST']\n",
                        "[]\n",
                        "['HRTX']\n",
                        "[]\n",
                        "['Shorted', 'NFLX']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Invo', 'Biosciences', '(IVOB)']\n",
                        "['[Discussion]']\n",
                        "['EUR/USD:', 'Strongest']\n",
                        "['GERN']\n",
                        "['$GCI']\n",
                        "[]\n",
                        "['$BGMD']\n",
                        "[]\n",
                        "['Guys']\n",
                        "['BLFS']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Looks']\n",
                        "['Paypal', 'Brokers?']\n",
                        "['$AXPW']\n",
                        "['TSLA']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['NBG']\n",
                        "[]\n",
                        "[]\n",
                        "['$GDP']\n",
                        "['[AAPL]']\n",
                        "['Going']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Well,']\n",
                        "['MSFT']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$LOCK,', '$3']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['[Discussion]']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['AAPL']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$AMEX']\n",
                        "['$AMMX']\n",
                        "[]\n",
                        "['MM']\n",
                        "[]\n",
                        "['TPIV']\n",
                        "[]\n",
                        "['Remember']\n",
                        "[]\n",
                        "['$V']\n",
                        "['Bigtime']\n",
                        "[]\n",
                        "['$SLTD', 'BITCHESS']\n",
                        "['Lifelock']\n",
                        "['QCOM,', 'MCD,', 'LUV,', 'UA,']\n",
                        "[]\n",
                        "['$APA']\n",
                        "['Puts']\n",
                        "['Caesars']\n",
                        "['Coupons,', 'Coupons,']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Avxl,']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['[Discussion]']\n",
                        "[]\n",
                        "['MSFT']\n",
                        "['CHK']\n",
                        "[]\n",
                        "['XOMA']\n",
                        "['MNGA']\n",
                        "[]\n",
                        "['Looking']\n",
                        "[]\n",
                        "['Damnit', 'MCD']\n",
                        "[]\n",
                        "[]\n",
                        "['AAPL', 'CALLS']\n",
                        "[]\n",
                        "['LPT:', 'Verticals']\n",
                        "[]\n",
                        "[]\n",
                        "['LPTN']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['BGMD']\n",
                        "[]\n",
                        "[]\n",
                        "['AMZN']\n",
                        "['Swapping']\n",
                        "['VRX', 'CALLS']\n",
                        "[]\n",
                        "['Amazon']\n",
                        "[]\n",
                        "['$AMZN']\n",
                        "[]\n",
                        "['$AMDA']\n",
                        "[]\n",
                        "['SNSS']\n",
                        "['$AMDA']\n",
                        "['$MNKD', 'Breakout']\n",
                        "['MPO']\n",
                        "[]\n",
                        "[]\n",
                        "['[Discussion]']\n",
                        "['AMDA']\n",
                        "['Fuck', '$AUD!', '@AUD/JPY', 'Chian']\n",
                        "['Fuck']\n",
                        "[]\n",
                        "['$AMDA']\n",
                        "['CBAY']\n",
                        "[]\n",
                        "['MNGA']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$AMDA']\n",
                        "['AAPL']\n",
                        "[]\n",
                        "[]\n",
                        "['Pandora', 'Yolo']\n",
                        "['AMDA']\n",
                        "['Remember']\n",
                        "[]\n",
                        "['BGMD']\n",
                        "['Thoughts']\n",
                        "['Selling']\n",
                        "[]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "source": [
                "#Simplified version - if words starts with $ or has multiple uppers in a row then add to list\n",
                "for row in sample_data:\n",
                "    probable_tickers_and_names = []\n",
                "    all_text = str(row[1]) + \" \" + str(row[2])\n",
                "    all_text = all_text.split()\n",
                "    for word in all_text:\n",
                "        if word[0] == \"$\":\n",
                "            probable_tickers_and_names.append(word)\n",
                "        elif word.isupper():\n",
                "            probable_tickers_and_names.append(word)\n",
                "        \n",
                "\n",
                "    #add key value pair to dictionary row number key then list\n",
                "    extracted_tickers_names[counter] = probable_tickers_and_names\n",
                "    counter += 1\n",
                "\n",
                "for x in range(200):\n",
                "    print(extracted_tickers_names[x])\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "['$MSFT']\n",
                        "[]\n",
                        "['$TOPS']\n",
                        "[]\n",
                        "['PBIB?', 'Ladies']\n",
                        "[]\n",
                        "['{BUY}']\n",
                        "['Whats']\n",
                        "['[Discussion]']\n",
                        "[]\n",
                        "[]\n",
                        "['Dgaz']\n",
                        "[]\n",
                        "['Calling']\n",
                        "[]\n",
                        "['EXEL', 'Posts']\n",
                        "['Analysis:']\n",
                        "['Vivint']\n",
                        "['AMD', 'UPDATE:']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$PYPL:']\n",
                        "['AMBA']\n",
                        "['EXXI', 'YOLO?']\n",
                        "['$rgse', 'Undervalued']\n",
                        "['$CYCC:']\n",
                        "['CODI', '$16.65', '--NEW']\n",
                        "['Picks']\n",
                        "['$NXGA']\n",
                        "[]\n",
                        "['$AVXL']\n",
                        "[]\n",
                        "[]\n",
                        "['HJOE']\n",
                        "['Jordan', 'Spieth']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$WLCDF']\n",
                        "['JDST']\n",
                        "[]\n",
                        "['HRTX']\n",
                        "[]\n",
                        "['Shorted', 'NFLX']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Invo', 'Biosciences', '(IVOB)']\n",
                        "['[Discussion]']\n",
                        "['EUR/USD:', 'Strongest']\n",
                        "['GERN']\n",
                        "['$GCI']\n",
                        "[]\n",
                        "['$BGMD']\n",
                        "[]\n",
                        "['Guys']\n",
                        "['BLFS']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Looks']\n",
                        "['Paypal', 'Brokers?']\n",
                        "['$AXPW']\n",
                        "['TSLA']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['NBG']\n",
                        "[]\n",
                        "[]\n",
                        "['$GDP']\n",
                        "['[AAPL]']\n",
                        "['Going']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Well,']\n",
                        "['MSFT']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$LOCK,', '$3']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['[Discussion]']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['AAPL']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$AMEX']\n",
                        "['$AMMX']\n",
                        "[]\n",
                        "['MM']\n",
                        "[]\n",
                        "['TPIV']\n",
                        "[]\n",
                        "['Remember']\n",
                        "[]\n",
                        "['$V']\n",
                        "['Bigtime']\n",
                        "[]\n",
                        "['$SLTD', 'BITCHESS']\n",
                        "['Lifelock']\n",
                        "['QCOM,', 'MCD,', 'LUV,', 'UA,']\n",
                        "[]\n",
                        "['$APA']\n",
                        "['Puts']\n",
                        "['Caesars']\n",
                        "['Coupons,', 'Coupons,']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['Avxl,']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['[Discussion]']\n",
                        "[]\n",
                        "['MSFT']\n",
                        "['CHK']\n",
                        "[]\n",
                        "['XOMA']\n",
                        "['MNGA']\n",
                        "[]\n",
                        "['Looking']\n",
                        "[]\n",
                        "['Damnit', 'MCD']\n",
                        "[]\n",
                        "[]\n",
                        "['AAPL', 'CALLS']\n",
                        "[]\n",
                        "['LPT:', 'Verticals']\n",
                        "[]\n",
                        "[]\n",
                        "['LPTN']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['BGMD']\n",
                        "[]\n",
                        "[]\n",
                        "['AMZN']\n",
                        "['Swapping']\n",
                        "['VRX', 'CALLS']\n",
                        "[]\n",
                        "['Amazon']\n",
                        "[]\n",
                        "['$AMZN']\n",
                        "[]\n",
                        "['$AMDA']\n",
                        "[]\n",
                        "['SNSS']\n",
                        "['$AMDA']\n",
                        "['$MNKD', 'Breakout']\n",
                        "['MPO']\n",
                        "[]\n",
                        "[]\n",
                        "['[Discussion]']\n",
                        "['AMDA']\n",
                        "['Fuck', '$AUD!', '@AUD/JPY', 'Chian']\n",
                        "['Fuck']\n",
                        "[]\n",
                        "['$AMDA']\n",
                        "['CBAY']\n",
                        "[]\n",
                        "['MNGA']\n",
                        "[]\n",
                        "[]\n",
                        "[]\n",
                        "['$AMDA']\n",
                        "['AAPL']\n",
                        "[]\n",
                        "[]\n",
                        "['Pandora', 'Yolo']\n",
                        "['AMDA']\n",
                        "['Remember']\n",
                        "[]\n",
                        "['BGMD']\n",
                        "['Thoughts']\n",
                        "['Selling']\n",
                        "[]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# Even simpler method. Can I use yahoo_fin to lookup each word and see if it's a ticker then append?\n",
                "\n",
                "for row in sample_data:\n",
                "    probable_tickers_and_names = []\n",
                "    all_text = str(row[1]) + \" \" + str(row[2])\n",
                "    regexpattern = r'[' + string.punctuation + ']' \n",
                "    all_text = re.sub(regexpattern, '', all_text)\n",
                "    all_text = all_text.split()\n",
                "\n",
                "\n",
                "\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 201,
            "source": [
                "# #Matching for tickers only. Names need to be partial ratio\n",
                "# possible_matches = []\n",
                "# for word in sample_data:\n",
                "#     for item in tickers_names_dict:\n",
                "#         similarity = fuzz.ratio(word, item)\n",
                "#         if similarity > 90:\n",
                "#             possible_matches.append([word, item, similarity ])\n",
                "\n",
                "#Retain index, ticker and name for extraction in a new list for upload to SQL\n",
                "\n",
                "# extracted_tickers_names = []\n",
                "# for row in sample_data[0:20]:\n",
                "#     possible_matches = [row[0]]\n",
                "#     for word in [row[1].split()]:\n",
                "#         for item in tickers_names_dict:\n",
                "#             similarity = fuzz.ratio(word, item[1])\n",
                "#             #if similarity > 90:\n",
                "#             possible_matches.append([word, item[1], similarity])\n",
                "#     extracted_tickers_names.append(possible_matches)\n"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.6.14",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6.14 64-bit"
        },
        "interpreter": {
            "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}